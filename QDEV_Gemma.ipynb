{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpvEqReJl2T4"
      },
      "outputs": [],
      "source": [
        "# Clone the BARTScore repository\n",
        "!git clone https://github.com/neulab/BARTScore.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/BARTScore"
      ],
      "metadata": {
        "id": "0dR_89BGmJKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from bart_score import BARTScorer\n",
        "import torch"
      ],
      "metadata": {
        "id": "weP51kxhmLl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install --upgrade pip  # ensures that pip is current\n",
        "!pip install tensorflow tensorflow_hub transformers\n",
        "!pip install git+https://github.com/google-research/bleurt.git\n",
        "!pip install pandas\n",
        "!pip install groq\n",
        "!pip install scikit-learn\n",
        "!pip install matplotlib\n",
        "!pip install nltk\n",
        "!pip install rouge\n",
        "!pip install sacrebleu\n",
        "!pip install bert-score\n",
        "!pip install rouge\n",
        "# Navigate to the repository directory\n",
        "%cd /content/BARTScore\n",
        "\n",
        "# Download and unzip BLEURT checkpoint\n",
        "!wget https://storage.googleapis.com/bleurt-oss-21/BLEURT-20.zip\n",
        "!unzip BLEURT-20.zip\n",
        "\n",
        "# Initialize BART-Scorer\n",
        "bart_scorer = BARTScorer(device='cuda:0' if torch.cuda.is_available() else 'cpu',\n",
        "                         checkpoint='facebook/bart-large-cnn')\n",
        "\n",
        "!pip install groq\n",
        "!pip install sentence-transformers\n",
        "!pip install chromadb\n",
        "!pip install pymongo pandas\n",
        "!pip install transformers torch accelerate bitsandbytes\n",
        "!pip install --upgrade transformers\n",
        "!pip install rouge\n",
        "!pip install google-search-results\n",
        "!pip install requests beautifulsoup4 lxml"
      ],
      "metadata": {
        "id": "AvwnlT1EmNwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import traceback\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import random\n",
        "import nltk\n",
        "from groq import Groq\n",
        "import pandas as pd\n",
        "import signal\n",
        "import time\n",
        "import re\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from sacrebleu.metrics import CHRF, TER\n",
        "from bert_score import score\n",
        "from bleurt import score as bleurt_score\n",
        "import json\n",
        "import random\n",
        "import warnings\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge import Rouge\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "8zqge8N1mQXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import pandas as pd\n",
        "import random\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import nltk\n",
        "import signal\n",
        "import time\n",
        "import traceback\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Suppress specific warning messages\n",
        "logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.ERROR)\n",
        "\n",
        "# Function to read and combine datasets\n",
        "def load_and_combine_datasets(file_paths):\n",
        "    combined_claims = []\n",
        "    for file_path in file_paths:\n",
        "        try:\n",
        "            data = pd.read_json(file_path)\n",
        "            combined_claims.extend(data.to_dict(orient='records'))\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading {file_path}: {e}\")\n",
        "    random.shuffle(combined_claims)  # Shuffle claims for randomness\n",
        "    return pd.DataFrame(combined_claims)\n",
        "\n",
        "# File paths for datasets\n",
        "file_paths = [\"/content/last_fake_claims.json\", \"/content/Modified_true_claims.json\"]\n",
        "\n",
        "# Load and combine datasets\n",
        "df = load_and_combine_datasets(file_paths)\n",
        "\n",
        "# Initialize the Groq API client with the first API key\n",
        "api_key = \"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\"\n",
        "client = Groq(api_key=api_key)\n",
        "\n",
        "# Label map\n",
        "label_map = {'true': 'true', 'fake': 'fake', \"correct\": \"true\", \"false\": \"fake\", \"incorrect\": \"fake\", \"not true\": \"fake\", \"Answer: FAKE\" : \"fake\", \"Answer: TRUE\" : \"true\", }\n",
        "\n",
        "class TimeoutException(Exception):\n",
        "    pass\n",
        "\n",
        "def timeout_handler(signum, frame):\n",
        "    raise TimeoutException\n",
        "\n",
        "# Set the timeout handler\n",
        "signal.signal(signal.SIGALRM, timeout_handler)\n",
        "\n",
        "def get_claim_predictions(claim, contextQuestions, timeout=120, retries=3):\n",
        "    attempt = 0\n",
        "    while attempt < retries:\n",
        "        try:\n",
        "            # Set the alarm for the timeout\n",
        "            signal.alarm(timeout)\n",
        "\n",
        "            prompt = f\"Label the following claim as 'true' or 'fake' from the given context. Answer with either 'TRUE' or 'FAKE' only in the first line. Also add another line for justification from the given context only. Context: {contextQuestions} . Claim:{claim}\"\n",
        "            print(f\"Input token length: {len(prompt.split())}\")\n",
        "\n",
        "            # Call the model with the prompt\n",
        "            chat_completion = client.chat.completions.create(\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                model=\"gemma2-9b-it\",\n",
        "                max_tokens=8000,\n",
        "            )\n",
        "\n",
        "            # Get the classification result from the model\n",
        "            predicted_result = chat_completion.choices[0].message.content.splitlines()\n",
        "            print(predicted_result)\n",
        "            predicted_label = re.sub(r'[^a-zA-Z]+', '', predicted_result[0]).lower()\n",
        "            predicted_label = label_map.get(predicted_label, \"unknown\")\n",
        "            predicted_justification = \" \".join(predicted_result[1:]).strip().lower()\n",
        "\n",
        "            # Cancel the alarm if the function completes in time\n",
        "            signal.alarm(0)\n",
        "\n",
        "            return (prompt, predicted_label, predicted_justification)\n",
        "\n",
        "        except TimeoutException:\n",
        "            print(f\"Timeout occurred for claim: {claim}, attempt {attempt + 1}\")\n",
        "            attempt += 1\n",
        "            if attempt < retries:\n",
        "                print(\"Retrying after 2 minutes...\")\n",
        "                time.sleep(30)  # Wait for 2 minutes before retrying\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing claim '{claim}': {e}\")\n",
        "            traceback.print_exc()\n",
        "            return (\"\", \"\", \"\")\n",
        "\n",
        "    return (\"\", \"\", \"\")\n",
        "\n",
        "# Initialize lists for results\n",
        "results = []\n",
        "actual_labels = []\n",
        "predicted_labels = []\n",
        "predicted_justifications = []\n",
        "actual_justifications = []\n",
        "\n",
        "# Initialize counters for tracking model performance\n",
        "correct_predictions = 0\n",
        "n_claims = 5006  # Adjust this to the desired number of claims to process\n",
        "total_processed_claims = 0\n",
        "\n",
        "# Initialize label map\n",
        "label_map = {'true': 'true', 'fake': 'fake', \"correct\": \"true\", \"false\": \"fake\", \"incorrect\": \"fake\", \"not true\": \"fake\"}\n",
        "\n",
        "for index, row in df.head(n_claims).iterrows():\n",
        "    claim = row['claim']\n",
        "    actual_label = str(row['label'].strip().lower())\n",
        "    actual_justification = str(row['justification'].strip().lower())\n",
        "    contextQuestions = \"\"\n",
        "    questions = row['questions']\n",
        "    for i, q in enumerate(questions):\n",
        "        contextQuestions += str(i+1) + q[\"question\"] + \"\\nAnswers:\" + str(q[\"answers\"])\n",
        "\n",
        "    prompt, predicted_label, predicted_justification = get_claim_predictions(claim, contextQuestions)\n",
        "\n",
        "    total_processed_claims += 1\n",
        "\n",
        "    if (predicted_label, predicted_justification) == (\"\", \"\"):\n",
        "        continue\n",
        "\n",
        "    actual_labels.append(actual_label)\n",
        "    actual_justifications.append(actual_justification)\n",
        "    predicted_labels.append(predicted_label)\n",
        "    predicted_justifications.append(predicted_justification)\n",
        "\n",
        "    if predicted_label == actual_label:\n",
        "        correct_predictions += 1\n",
        "\n",
        "    print(f\"Claim #{total_processed_claims}\")\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    print(f\"Claim: {claim}\")\n",
        "    print(f\"Predicted Label: {predicted_label}\")\n",
        "    print(f\"Actual Label: {actual_label}\")\n",
        "    print(f\"Predicted Justification: {predicted_justification}\")\n",
        "    print(f\"Actual Justification: {actual_justification}\")\n",
        "    print('-' * 50)\n",
        "\n",
        "# Calculate the overall accuracy of the model\n",
        "accuracy = (correct_predictions / total_processed_claims) * 100 if total_processed_claims > 0 else 0\n",
        "print(f\"Model Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(actual_labels, predicted_labels, labels=[\"true\", \"fake\"])\n",
        "\n",
        "# Display the confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"true\", \"fake\"])\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lxxDlBAJmWVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Metrics initialization\n",
        "rouge = Rouge()\n",
        "smoothing_function = SmoothingFunction().method4\n",
        "chrf = CHRF()\n",
        "ter = TER()\n",
        "bleurt_scorer = bleurt_score.BleurtScorer(\"BLEURT-20\")\n",
        "\n",
        "# Filter out empty justifications\n",
        "filtered_justifications = [\n",
        "    (actual, predicted)\n",
        "    for actual, predicted in zip(actual_justifications, predicted_justifications)\n",
        "    if predicted.strip()\n",
        "]\n",
        "\n",
        "# Calculate metrics\n",
        "def calculate_metrics_extended(justifications):\n",
        "    bleu_scores = []\n",
        "    rouge_scores = []\n",
        "    meteor_scores = []\n",
        "    chrf_scores = []\n",
        "    ter_scores = []\n",
        "    bert_scores = []\n",
        "    bleurt_scores = []\n",
        "    bart_scores = []\n",
        "\n",
        "    for ref, pred in justifications:\n",
        "        # BLEU score\n",
        "        bleu = sentence_bleu([ref.split()], pred.split(), smoothing_function=smoothing_function)\n",
        "        bleu_scores.append(bleu)\n",
        "\n",
        "        # ROUGE score\n",
        "        rouge_score = rouge.get_scores(pred, ref, avg=True)\n",
        "        rouge_scores.append(rouge_score)\n",
        "\n",
        "        # METEOR score\n",
        "        meteor = meteor_score([ref.split()], pred.split())\n",
        "        meteor_scores.append(meteor)\n",
        "\n",
        "        # ChrF\n",
        "        chrf_scores.append(chrf.corpus_score([pred], [[ref]]).score)\n",
        "\n",
        "        # TER\n",
        "        ter_scores.append(ter.corpus_score([pred], [[ref]]).score)\n",
        "\n",
        "        # BERTScore\n",
        "        P, R, F1 = score([pred], [ref], lang=\"en\", device=device)\n",
        "        bert_scores.append(F1.mean().item())\n",
        "\n",
        "        # BLEURT\n",
        "        bleurt_scores.append(bleurt_scorer.score(references=[ref], candidates=[pred])[0])\n",
        "\n",
        "        # BARTScore\n",
        "        bart_scores.append(bart_scorer.score([pred], [ref])[0])\n",
        "\n",
        "    return bleu_scores, rouge_scores, meteor_scores, chrf_scores, ter_scores, bert_scores, bleurt_scores, bart_scores\n",
        "\n",
        "# Calculate metrics\n",
        "bleu_scores, rouge_scores, meteor_scores, chrf_scores, ter_scores, bert_scores, bleurt_scores, bart_scores = calculate_metrics_extended(filtered_justifications)\n",
        "\n",
        "# Summarize and print results\n",
        "avg_rouge_1 = sum([score['rouge-1']['f'] for score in rouge_scores]) / len(rouge_scores)\n",
        "avg_rouge_2 = sum([score['rouge-2']['f'] for score in rouge_scores]) / len(rouge_scores)\n",
        "avg_rouge_l = sum([score['rouge-l']['f'] for score in rouge_scores]) / len(rouge_scores)\n",
        "\n",
        "print(f\"Average BLEU Score: {sum(bleu_scores) / len(bleu_scores):.4f}\")\n",
        "print(f\"Average METEOR Score: {sum(meteor_scores) / len(meteor_scores):.4f}\")\n",
        "print(f\"Average ChrF Score: {sum(chrf_scores) / len(chrf_scores):.4f}\")\n",
        "print(f\"Average TER Score: {sum(ter_scores) / len(ter_scores):.4f}\")\n",
        "print(f\"Average BERTScore F1: {sum(bert_scores) / len(bert_scores):.4f}\")\n",
        "print(f\"Average BLEURT Score: {sum(bleurt_scores) / len(bleurt_scores):.4f}\")\n",
        "print(f\"Average BARTScore: {sum(bart_scores) / len(bart_scores):.4f}\")\n",
        "print(f\"Average ROUGE-1 F1 Score: {avg_rouge_1:.4f}\")\n",
        "print(f\"Average ROUGE-2 F1 Score: {avg_rouge_2:.4f}\")\n",
        "print(f\"Average ROUGE-L F1 Score: {avg_rouge_l:.4f}\")\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = (correct_predictions / total_processed_claims) * 100 if total_processed_claims > 0 else 0\n",
        "print(f\"Model Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# Calculate the F1 score\n",
        "f1 = f1_score(actual_labels, predicted_labels, average='weighted', labels=[\"true\", \"fake\"])\n",
        "print(f\"F1 Score Average: {f1}\")\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(actual_labels, predicted_labels, labels=[\"true\", \"fake\"])\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"true\", \"fake\"])\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uHO_a3NhmaaF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}